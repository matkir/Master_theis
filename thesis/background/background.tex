\section{Machine Learning}
    Machine learning is a very broad term, but can i short be summarised by:\\
    \vspace{10px}
	
    \textit{ A computer program is said to learn from experience E with respect to 
    some class of tasks T and performance measure P, if its performance at
    tasks in T, as measured by P, improves with the experience E. } 
    \cite{MitchellTomM1997Ml}\\
	
    \vspace{10px}
    Here we have a couple of parameters:\\
    \textbf{E} text about e\\
    \textbf{T} text about t\\
    \textbf{P} text about p\\
	
    From this we see that the goal of machine learning is to improve some performance P with experience.
    \textbf{might here talk about different tasks ML can do?}

    \subsection{How machine learning works}   
	\subsubsection{Feed forward}
    
	\subsubsection{Loss and gradient decent }

    
    \subsection{Supervised \& Unsupervised machine learning}
	We often divide machine learning in to two (diffuse) categories: supervised and unsupervised.\\
	\vspace{5px}
	\textbf{Supervised learning:} is the act of training with data that has an answer or a label. The learning algorithm can get supervision while 
	training on the task. An example on a supervised task  is to recognise handwritten numbers, or differentiate between dogs and cats. The task is supervised if the images
	comes with the correct label in the data set. These  examples are typical classification examples, where the task is to identify the right group to classify the data to %TODO more
	A simpler classification assignment is binary classification, where the target is (often) yes or no. Examples for binary classification is if an email is spam or not, is a car Norwegian 
	or International. 
	In the last example the classification changes from binary to multi-class if you sort the cars on every nationality, and not just Norwegian/non-Norwegian.
	  
	Another type of supervised learning is regression. This is the act of prediction given prior data. Examples of regression is everything from prediction of stock prices, to house prices 
	in an area, to\\ %TODO more
	\begin{figure}
	    \centering
	    \includegraphics[scale=0.5]{figures/class_vs_reg.png}
	    \caption{Left: Example of binary classification. Right: Example of regression} 
	\end{figure}
  
	\vspace{5px}
	\textbf{Unsupervised learning:} is the act of training without any supervision, on the sense that we do not give the algorithm the answer
	to the training data set. %TODO more 
	 
	Since we do not have categorised data in unsupervised learning, we often %TODO more
	Types of unsupervised learning can for instance be clustering, the act of sorting data based on similarity. An example of this can be if you want to sort plants based on species, or 
	you are detecting anomalies in a dataset.
	Unsupervised learning can be used for PCA %TODO CITE 
	or other dimensionaly reduction methods.\\
	  
	A third method to used unsupervised learning is the adversarial route, where you use machine learning to make similar looking data to the original data set. 
	    
	\begin{figure}
	    \centering
	    \includegraphics[scale=0.5]{figures/cluster_pca.png}
	    \caption{Left: Example of binary clustering. Right: Example of principal component analysis} 
	\end{figure}

	 
	In the description of supervised vs unsupervised we looked at a specific branch of machine learning: Classification. Classification is, as the name implies, the task of 
	getting data sorted in to groups of similarity. 
	  
	  
	\begin{itemize}
	    \item subsfication
	    \item r to the pillcam projression 
	    \item transcription/translation
	    \item de-noising /finding missing inputs
	\end{itemize}
	  
    \subsection{Types of machine learning (AKA what we can do with ML)}
	There are a number of different machine learning algorithms. %TODO tell that we are going inn to detail here
	\begin{table}[ht]
	    \centering
	    \resizebox{\textwidth}{!}{%
	    \begin{tabular}{|c|c|c|c|c|}
	      \hline
	      \multicolumn{5}{|c|}{Machine Learning}                                                                                 \\ \hline
	      \multicolumn{2}{|c|}{Supervised Learning}   & \multicolumn{2}{c|}{Unsupervised Learning}       & Reinforcement Learning\\ \hline
	      Classification          & Regression        & Clustering           & Dimensionality reduction  & -                     \\ \hline
	      Support vector machines & Linear Regression & K means clustering   & PCA                       & SOMething             \\
	      K nearest neighbours    & Decision trees    & Hidden Markov models &                           &                       \\
	      Neural networks         & Neural networks   & Neural Networks      &                           &                      
	    \end{tabular}%
	    }
	    \caption{Machine leaning types}
	    \label{ML-types}
	  \end{table}
	  
	  %TODO rewrite under.
	  
	  \vspace{5px}
	  \textbf{K nearest neighbours}\\
	  Talk about KNN\\
		
	  \vspace{5px}
	  \textbf{Linear Regression}\\
	  How to regress linearly\\
	  
	  \vspace{5px}
	  \textbf{Support vector machine}\\
	  SVM and 2 class\\
	  
	  \vspace{5px}
	  \textbf{Others?}\\
	  Other important ones to talk about?\\
	
	
	  \vspace{5px}
	  \textbf{Neural networks}\\
	  %TODO har given good results last years
	  %leading in the field
	  %own chapter
	  NN is future\\
	  own chapter\\
	  

	
\section{Neural Networks}
	  
    \subsection{How it works}
	    %TODO talk about backward/forward prop?
	    %
	
	  
    \subsection{Convolutional neural networks}
	  
    \subsection{Advaserial neural networks}
  	  	\paragraph{This is explaining GANS, put me in the right place} %%TODO CITE: http://www.deeplearningbook.org/contents/generative_models.html and Goodfellow at al. 2014
	  Now that we have looked at autoencoders we can take it a step further. 
	  generative advaserial models can be used as a generator of new data, and can have som reseblance to autoencoders \ref{Explaining_autoencoders}, especially variational autoencoders %TODO cite VAE, ref VAE
	  
	  The difference lies in that advaserial networks is based on game theoretic scenarios in which a generator network is compeating agenst an advasery. 
	  The generator produces samples $x=g(z;\theta^{(g)})$, where $g$ is the network given the weights $\theta$. Then the discriminator network predicts if a sample is drawn from the dataset or from the generator.
	  More spessific, it gives a probably given by $d(x;\theta^{(d)})$ , and determins if $x$ is from the generator or the data-set. 
	  Since we have two networks compeating agenst each other we can look at this as a Zero-sum game with the generators payoff is determined by $v(\theta^{(g)},\theta^{(d)})$, and the discriminators payoff is determined 
	  by $-v(\theta^{(g)},\theta^{(d)})$.
	  \textit{$v$ is here a function that is determined by both the sucsess rate of the discriminator and the generator, the most common used is}
	  \begin{equation}
	  v(\theta^{(g)},\theta^{(d)}) \; = \; \mathds{E}_{x\sim p_{data}}\log{d(x)} + \mathds{E}_{x\sim p_{model}}\log{(1 - d(x))} %TODO: CITE First gan paper on formula
	  \end{equation}
	  as derived from Goodfellow et al. %TODO cite 
	  
	  Lets look at a gan in detail. \\
	  \begin{figure}[ht!]
	    \centering
	    \includegraphics[scale=0.5]{background/figures/simpleGAN.png}
	    \caption{The idea behind a GAN. Here the generator saples from a random (Gaussian) distribution and generates samples that the discriminator classifies as real or fake}
	\end{figure}
	
	 %TODO MORE
    \subsubsection{UCNN?}	
	
	
\section{Models we need to explain at this point (find better tittle)}
    \subsection{Autoencoders}\label{Explaining_autoencoders}
	  %%TODO CITE: http://www.deeplearningbook.org/contents/autoencoders.html
	  As we recall from earlier, an autoencoder is a type of neural network that tries to output a recreation of the output.\todo{we are not recalling} \\ 
	  
	  We can do this by having an encoder, $h=f(x)$, connected to a decoder, $r=g(h)$. 
	  An autoencoder has the job to set $g(f(x))=x$ over the whole input, but in most cases this is not a practical program. We often gives the autoencoder the restriction
	  that it has to map the input through a latent space that has a smaller dimension than the input dataset.\\
	  This is called an undercomplete autoencoder.\\
	  \vspace{10px}
	  \begin{figure}[ht!]
	    \centering
	    \includegraphics[scale=0.5]{background/figures/SimpleAE.png}
	    \caption{The general structure of an autoencoder, mapping $\textbf{x}$ through $\textbf{h}$ to an output $\textbf{r}$.}
	  \end{figure}
	  
	  As with supervised classifiers we can use gradient decent to optimize the model. This is because we are trying to recreate the input $\textbf{x}$ from out output $\widetilde{\textbf{x}}$\\
	  
	  This can simply be done by minimizeing the loss function\\
	  \begin{equation}
	    L(\textbf{x},g(f(\textbf{x})))
	  \end{equation}
	  with for instance MSE with gradient decent. \todo{explain MSE, and general loss on an ealier time}\\
	  
	  Now we can transfer this to a more relevant example by making an image as input and use convolutions to reduce the dimensionality in the encoder and increase the dimentionality in the encoder.
	  \vspace{10px}
	  \begin{figure}[ht!]
	    \centering
	    \includegraphics[scale=0.5]{background/figures/CAE.png}
	    \caption{Convolutional autoencoder with an RGB image as input, and the reconstructed image as output.}
	  \end{figure}
	
    
    
    \subsection{Contextencoders}
	Inpainting can also be done with advaserial models, and using a network trained to do the task of inpainting can be a lot more powerful than using just an autoencoder\todo{ref} or the naive methods\todo{ref}.
	A contextencoder is building on the advaserial principle by using a generator/discriminator pair to fill in masked areas in an image. 
	
	The concept behind a Contextencoder is to take the whole image as input to an encoder/decoder pair and \todo{finish}
    
    
    \subsection{CC-GANS}
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
      HERE IS TEXT ABOUT CCGANS
    
    
    \subsection{Pixel CNN}
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
      HERE is text about pccn
    


\section{Cancer and polyps}
	  \subsection{What we are looking for REM}
	  Different types of disorders.
	  %TODO image of polyp
	  Polyp is harmless, but if left untreated it can become cancerous
	  %TODO tell the risk
	  Pictures is from the pillcam project, kvasir dataset.
	  \subsection{images from pillcam, and what we are looking at/for REM}
	  %TODO More pictures of polyps, and other anomalies
	  

	  
	  

	    
	  
\section{Explain how the ML-methods can be used with the polyps}
	basicly talk about detecting polyps
	
	  
	  
	  
\section{The problem at hand}
	  Now that we have the definition of machine learning and the current task, we can focus on the task at hand; finding polyps. In an ideal world we have a
	  Classification problem with only two classes: Non-polyp and polyp. 
	  
	  \begin{itemize}
	    \item SVM 
	    \item CNN 
	    \item random forests
	    \item knn
	  \end{itemize}
	  
	
	\input{background/inpaining_naive_ML.tex}