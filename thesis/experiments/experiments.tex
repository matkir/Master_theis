In the previous chapter, we described our methodology. We presented the framework we used for our setup, and we went into detail on the configuration of the experiments, and the project as a whole describing both the masking and the neural networks. 
In this chapter, we will start with the description of the datasets used on the results and the metrics chosen as a reference point for the evaluation of our results. 
Finally, for each model, we will evaluate it for the datasets we have presented.

\section{Datasets}
To show and explain the experiments, we need to look at the datasets we have to evaluate our results. 
\todo{something about Simula already researching on this}

In this thesis, we primarily use four datasets to train and evaluate our results.



\subsection{kvasir}



As we recall from the introduction chapter, oesophagal, stomach and colorectal cancer accounts for about 2.8 million new cases and 1.8 million deaths per year, and this number is increasing as the population gets older.   With automatic detection of diseases by use of computers being prominent, but a still unexplored field of research the dataset Kvasir was made.
Kvasir is a Multi-Class Image Dataset for Computer Aided Gastrointestinal Disease Detection made in Norway. The data is gathered from the Vestre Viken Health Trust, and it contains not only polyps but also two other findings, two classes related to polyp removal and three anatomical landmarks in the GI tract.

The data is collected using endoscopic equipment at Vestre Viken Health Trust in Norway. The Vestre Viken Health Trust is a collection of 4 hospitals, and it provides health care to 470.000 people.
One of the more prominent hospitals in this group is Bærum Hospital. It has a large gastroenterology department where the data originates, and more data will be provided in the future. The data from Bærum Hospital has been annotated by medical experts and the Cancer Registry of Norway before its inclusion in the Kvasir dataset, making the data thoroughly marked and checked.
The Cancer Registry of Norway work provides new knowledge about cancer prevention and cancer treatment. It is part of the South-Eastern Norway Regional Health Authority and organised as an independent institution under Oslo University Hospital Trust. The Cancer Registry of Norway is responsible for the national cancer screening programmes with the goal to ultimately prevent cancer death by discovering cancers or pre-cancerous lesions as early as possible. 



Kvasir is a dataset containing images from inside the gastrointestinal tract containing three anatomical landmarks, in the form of the Z-line (a), the Pylorus (b) and the Cecum (c). Also, the Kvasir dataset includes two categories of images related to endoscopic polyp removal, Dyed and Lifted Polyps (d) and Dyed Resection Margins (e), and lastly, three classes, Esophagitis (g), Polyps (h), and Ulcerative Colitis (i), containing images of pathological findings.

\subsubsection{Anatomical Landmarks}
The definition of an anatomical landmark is a feature that is easily recognisable through an endoscope. For the medical staff they are essential for navigating the GI tract, and the assist as a reference point both for a physical location, and to describe the position for a relevant finding.
It is often relevant for a complete endoscopic report to contain image documentation and a description of these three landmarks.
 
\paragraph{Z-line}
The Z-line is the sphincter between the oesophagus and the stomach. This sphincter is an endoscopically recognisable border where the white mucosa meets the red gastric mucosa from the stomach.  When performing endoscopy an assessment of the Z-line is important to determine whether disease is present or not. A malfunctioning sphincter can, for instance, lead to diseases like gastroesophageal reflux. \todo{cite}.
Figure \ref{fig:z-line} shows a normal z-line taken from an endoscopy. 

\textbf{Pylorus}
We call the barrier between the stomach and the upper part of the small bowel for the Pylorus. This sphincter is responsible for letting food from the stomach into the GI tract, and to keep the stomach acid from flowing into the bowels.
Medical staff inspect both sides of the pylorus when doing a complete colonoscopy and endoscopy. This inspection is crucial to identify findings like ulcerations, erosions or stenosis.  \todo{cite}
Figure \ref{fig:pylorus} shows an endoscopic image of a healthy pylorus taken from inside the stomach.

\textbf{Cecum}
The last anatomical landmark used in gastronomy is the cecum. It is an intraperitoneal pouch that is considered to be the beginning of the large intestine. This area in the GI tract is considered the final stop for a standard colonoscopy as it is the last quality indicator for the procedure.
Being the last landmark, the identification and inspection of this area are one of the most crucial steps in the whole inspection. 
Figure \ref{fig:cecum} shows a normal cecum.


\subsection{Pathological Findings}
Pathological findings are areas or objects with an abnormal or unwanted feature. When looking at it from the oral entrance, pathological findings takes the form of inflammation, while in the lower GI tract it takes the form of, for instance, polyps. 
Pathological findings are the group of images that are especially important to detect and classify as they impose the most significant risk for the patients well being. \todo{do not like the ending}


\paragraph{Esophagitis}
Esophagitis is an inflammation oesophagus near the Z-line. Usually, the oesophagus is composed of a mucosal lining and circular smooth muscle fibres, but when a patient has a condition like a gastroesophageal reflux disease the area can become inflamed. 
Clinically, detection of this inflammation is necessary for treatment initiation to relieve the symptoms and to prevent further development that can lead to more complications.


\paragraph{polyps}
Polyps are lesions in the bowel detectable as mucosal outgrowths. There are multiple types of polyps that are either flat, elevated, or pedunculated. Polyps are distinguishable from normal mucosa as they have a different colour and surface texture. Most of these bowel anomalies are harmless, but they have, as discussed in the introduction, the possibility to become cancerous. 
There is a big focus dedicated to the removal and detection of there polyps as they are a significant threat to cancer of not treated in time. Because of the different shape and rate of outgrowth, medical staff overlook a small, but significant part of the polyps during routine procedures. 
This rate of error was the most significant driving force for investing in computer-aided medical diagnosis, and subsequently creating the Kvasir dataset.

\paragraph{Ulcerative Colitis}
Ulcerative Colitis is the last pathological finding type in the Kvasir dataset. 
It is a chronic inflammatory disease affecting the large bowel. Ulcerative Colitis may both come from psychological and physiological factors, but the origin of the disease is often unknown. The degree of inflammation is a factor when performing medical diagnosis, though in the Kvasir dataset we only have one class of ulcerative colitis.
A severe example of ulcerative colitis can is seen in figure \ref{fig:ulcerativecolitis}. Here we have an evident inflammation, and thus this would most likely be classified as severe.



\subsection{Polyp Removal}
The last groups are concerned with the removal of polyps. A technique often used in polyp removal is endoscopic mucosal resection (EMR). \todo{cite}
This procedure includes the injection of a liquid underneath the polyp, lifting it from the underlying tissue. After the injection, the medical staff use a snare to wrap around remove the polyp. Using this method of lifting and snaring, the medical staff reduces the risk of damaging the surrounding tissue by a large margin.

\paragraph{Dyed and Lifted Polyps}
The first of the two classes concerning polyp removal is the Dyed and Lifted Polyps class. This case is the polyp after it has been lifted from the surrounding tissue, and the next step is to use a snare to remove it.
Figure \ref{fig:dyedandliftedpolyps} shows a polyp after the injection liquid is applied. 


\paragraph{Dyed Resection Margins}
The resection margin is important to evaluate after the removal of a polyp, as we want to be sure the entirety of the are is clean. 
If the procedure did not eradicate the polyp, the residual polyp tissue might lead to continued growth, and in the worst case malignancy development. 




%\subsection{Nerthus} 
\todo{do we do Nerthus?}
\subsection{CVC-356}
The 

\subsection{CVC-12k}
\subsection{CVC-612}


 
    
\newpage

%============================================

    \begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{experiments/images/dyed-lifted-polyps.jpg}
            \caption[Is this in use]%
            {{\small  }}    
            \label{fig:polypAEGREEN}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{experiments/images/dyed-resection-margins.jpg}
            \caption[Hate to be this guy]%
            {{\small }}    
            \label{fig:polypGAN}
        \end{subfigure}
        \qquad\vfill%\vskip\baselineskip
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/images/esophagitis.jpg}
            \caption[]%
            {{\small }}    
            \label{fig:zAE}
        \end{subfigure}
        \qquad%\hfill%\quad
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/images/normal-cecum.jpg}
            \caption[]%
            {{\small }}    
            \label{fig:zGAN}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{experiments/images/normal-pylorus.jpg}
            \caption[Is this in use]%
            {{\small  }}    
            \label{fig:polypAEGREEN}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{experiments/images/normal-z-line.jpg}
            \caption[Hate to be this guy]%
            {{\small }}    
            \label{fig:polypGAN}
        \end{subfigure}
        \qquad\vfill%\vskip\baselineskip
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/images/polyps.jpg}
            \caption[]%
            {{\small }}    
            \label{fig:zAE}
        \end{subfigure}
        \qquad%\hfill%\quad
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/images/ulcerative-colitis.jpg}
            \caption[]%
            {{\small }}    
            \label{fig:zGAN}
        \end{subfigure}
        \caption[ ]
        {\small New text here} 
        \label{fig:GC1GREEN}
    \end{figure*}
    
     



    %=============================================

\section{Metrics}
To discern the results of our experiments we introduce multiple metrics and tables to get an indication of our success. 
The main dataset we used for training, Kvasir, was split into k number of folds, using k-fold cross-validation. 
K-fold cross-validation is a tool used in statistics and machine learning to help to get an accurate representation of the data based on an average of the dataset.  In machine learning, it is a powerful tool that can help with adapting to new datasets and prevent overfitting. 
Take the Kvasir dataset containing 8000 images with 1000 images from each class. We can choose a value for the number of folds, 'k', to be 6. This means that we split our dataset into six pieces before training.  With the six pieces, we assign one of them as a test set, and we assign the four other for training and validation. We then train our data five times, using four folds for training and the last fold for validation during training. For each training run we rotate the validation set, so each of the five folds is used for validation once. \todo{img of k fold}

The advantage of this method is that we maximise the utility of the dataset. We find the distribution of the dataset that hopefully covers the most significant range of the unseen data. 
\subsection{presantation of data, confusion matrix}
With k-fold cross-validation, we end up with the dataset that scored the highest during the final validation step.  There are multiple ways to calculate metrics for how well a dataset is doing,  but they all are a comparison of the predicted class versus the true class.  
Take for instance the case with the 8-class dataset Kvasir, where we predict an image to be of class 3, which in the real world be normal-z-line, while the actual True class is 5, and this might be esophagitis.\todo{rewrite}

We can represent this as 
\begin{verbatim}
[3, 5] 
\end{verbatim}.
Storing value pairs like this can very quickly get cluttered and unorganised.
The most common way to store these value pairs is to use a confusion matrix.  We initiate the matrix as a \textit{NxN} matrix where N is the total number of classes.
\begin{verbatim}
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
\end{verbatim} 
After we have initisialised the confusion matrix, we add each value pair as to the matrix at is corresoponding postition.  Given the pair \begin{verbatim} [3, 5]  \end{verbatim} we add one to the posistion corresoponding to (x=3,y=5).  Another example could be given the pair where we guessed class 0 and the true class was 0 (\begin{verbatim}[0, 0] \end{verbatim}), we add one to the matrix at position (x=0,y=0). Witch 2 examples we get the following confusion matrix.
\begin{verbatim}
 [  1  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   1   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
 [  0  0   0   0   0   0   0   0]
\end{verbatim}
As we fill in the matrix with more predictions we can start to draw conclusions from it. After appoximently 1600 evaluations, our result might at the end look like this.
\begin{verbatim}
 [195  50   0   0   0   0   2   1]
 [  4 148   1   0   0   0   0   0]
 [  0   0 152   0   3  40   0   5]
 [  0   1   0 198   0   0  13   4]
 [  0   0   0   0 195   1   5   2]
 [  0   0  47   0   1 159   0   0]
 [  1   0   0   0   0   0 172   8]
 [  0   1   0   2   1   0   8 180]
\end{verbatim}
We can see that the highest numbers lies around the diagonal. This means that most of our results were classified correctly as values at the diagonal is the same x and y values, and subsequently a correct prediction. 
We can also discern something about the four primary metrics associated with a value in the matrix.\\

\textbf{True Positive (TP): } True positive for a specific class is when it is predicted positive, and the True label is also positive.  In the Kvasir dataset, we have a True positive result if, for the class polyp, we predict a polyp.\\

\textbf{True Negative (TN): } True Negative is the opposite of true positive. Given the class Polyp from the Kvasir dataset, we guess that the image is not a polyp when the True label is non-polyp. 

\textbf{False Positive (FP): } False positive is, given a True label we predict False. We often call this type of error a "Type 1 error".   In the polyp case, this is the case where we predict a polyp given no polyp present.


\textbf{False Negative (FN): } False positive is the case where we fail to predict the class when it is True. This type of error is often called "Type 2 error". False Negative is, in our case, the least desirable outcome for our classes with pathological findings like Esophagitis, Polyps and Ulcerative Colitis.

For our multiclass confusion table, we can look at the four metrics like this
\todo{https://i.stack.imgur.com/AuTKP.png}

%\begin{table}[]
%\begin{tabular}{llll}
%                            & \multicolumn{3}{l}{Predicted label} \\
%\multirow{3}{*}{True label} &            & True      & False      \\
%                            & True       & 5         & 7          \\
%                            & False      & 2         & 10        
%\end{tabular}
%\end{table}  

\subsection{common Metrics}
When evaluating our results, we use a set of common metrics used in the field of statistics and machine learning.  The metrics are Recall (REC), Precision (PREC), Specificity (SPEC), Accuracy (ACC), Matthews correlation coefficient (MCC), and F1 score (F1). 


\vspace{5px}
\textbf{Accuracy:}  Accuracy is one of the simplest metrics to understand. It describes how many of our predictions were correct out of the total predictions made. It is the most common metric given its simplicity both in calculation and understanding. 
In general when our data is balanced, and we only have a few classes, we can get away with using accuracy. 
In this project, we use accuracy during the training step as a metric of success.
 \begin{equation}
ACC=\frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

\vspace{5px}
\textbf{Recall:}  Recall is the probability of detection . In a binary classification set, it measures the proportion of actual positives that are correctly identified as such. 
For our medical image classification, this metric can help us understand how our algorithms work by looking at classes where we have small anomalies in the images, like the case with polyps.
%\todo{ "Detector Performance Analysis Using ROC Curves – MATLAB & Simulink Example". www.mathworks.com. Retrieved 11 August 2016.}
\begin{equation}
TPR=\frac{TP}{TP+FN}
\end{equation}

\vspace{5px}
\textbf{Specificity:} Specificity measures the proportions of our samples that were correctly identified as negative, when the true class were also negative. 
\begin{equation}
TPR=\frac{TN}{TN+FP}
\end{equation}

\vspace{5px}
\textbf{Precision:}



\vspace{5px}
\textbf{F1 score:}

\vspace{5px}
\textbf{Matthews correlation coefficient:}

\subsection{Singleclass vs Multiclass Metrics}

The metrics presented are, in general, a solid way to present the validity of a model. However, not all metrics presented is the same when switching between single and multiclass classification.  Metrics like Accuracy is designed to work for multiclass classification, given that there is only one way to calculate the score.
\begin{equation}
\frac{sum(diag(covariance_matrix))}{sum(covariance_matrix))}
\end{equation}

The problem with multiclass metrics arises when there is more than one way to calculate the metrics needed, this can be for instance Recall and Specificity, where we have multiple ways to add together the classwise scores. The three most common ways to calculate the average are:

\textbf{Micro average}: calculates the mean value of each of the binary metrics and averages the result over the total number of samples. 
Micro average ignores all class frequencies and gives us a metric based on all samples gathered. Micro-averaging may be preferred in multilabel settings, including multiclass classification where a majority class is to be ignored.\\

\textbf{Macro average}: calculates the mean of each of the binary metrics, giving the same weight to each of the classes. Macro average gives importance to classes with few samples, and infrequent classes play the same roles as frequent ones. The disadvantage with Macro average is the fact that in the real world some classes often plays a more significant role than others, and doing especially bad on one of the classes can worsen the total result. \\

\textbf{Weighted average} calculates the mean of each of the binary metrics but gives a weighted sum for each of the scores before it is averaged. 
The weight of each class depends on the size of the true data samples.
The weighted average gives us the advantage that small classes still count more than it would with for instance Micro average, but since it depends on the number of samples from each class it can end up more or less as a black box during calculation.
Weighted average gives us the best of both worlds, but it lacks the intuitiveness from the two other classes. \\

With the three methods presented, we have chosen to Macro average our results. While both Macro and Weighted average would give a good indication given that not all our datasets are balanced, we argue that the weighted average would give metrics that are harder to explain when we are working on datasets with unbalanced classes.

In addition to looking at the Macro average of precision and recall, we want to look at specific cases of the classification.  In many cases, we have multiple classes, where we are most interested in just one or a handful of the classes shown. 
For instance, a focus we have in this thesis is to give a score on how predictable polyp detection is, and on that case, we want to discuss the True positive rate (TPR) of the polyp detection and not the TPR of the non-polyp classes. 

Take for instance the matrix shown in \todo{fig}\\
\begin{verbatim}
[[10 1]
 [3 12]]
\end{verbatim}
Here we can calculate the weighted average recall to be \textbf{x}. This can be an interesting observation in itself, but often the first or second True label is much more important relative to the other.  In a more practical example: We are more interested in finding areas with polyps when we know they are present, compared knowing there is not a polyp in an area when none are present. 

These Metrics becomes a more prominent topic when it comes to inpainting. With inpainting, we take areas with no relevant information and makes it into areas that are similar to the rest of the image. Given that we can inpaint over polyps by mistake, or that we might train our classifiers to not look in certain areas when classifying, we have an interest if also comparing single cases of recall and precision included to the average values.






\section{Setup of experiments and hardware}
We propose the following hypothesis.
\vspace{10px}

\textit{
When classifying images, we will get the best result when we have images with the least amount of sparse information. 
Hence by removing areas with sparse information,
we will see an increase in classification performance compared to not removing areas.
}

\vspace{5px}
and

\vspace{5px}
\textit{
When training a classifier, we will get a higher
mode of generalisation of our results when removing the dataset
specific artefacts compared to not removing artefacts.}
\vspace{5px}

In this thesis, we will set up our experiments to test our two hypothesises. 
We divide our work into two parts, inpainting and classifying. 
First, we will look at the process of inpainting in detail, and inspect the results we have.  
We will look at how parameters affect the results, and how different networks will differ in the generating process. \todo{more}


After a rundown of the generation of the custom dataset trough inpainting, we will show how the classification scores. Here we look at the datasets generated by inpainting and compare them with a base case. 

Our primary goal in this thesis is to see if any of the generated datasets can help with classification. We will both compare the different areas inpainted, and the method used to inpaint the images. 
In addition to just comparing accuracy, we will also look especially at Recall of the polyp class, and the MCC score. 
As mentioned in chapter \todo{find MCC chapter} and the \todo{find dataset chapter}, we have datasets which are unbalanced, and subsequently, need a more representative score compared to F1 or accuracy. 
We can also recall that the recall score \todo{2x recall} gives us a  probability of detection in a binary class situation. We will use this to see if any one of the generated datasets modifies this score in any manner. 

We want to end this chapter with a small experiment to see if network types affect the recall of polyps if convolutions are used compared to dense connections. We will use this to conclude if it is the network type of the inpainting that has the best effect when it comes to a good recall score for polyps.


\textbf{hardware}\\
The Hardware used for the generation and calssification is as follows.
\begin{table}[h]
\caption{Hardware for things}
\begin{center}
\begin{tabular}{ll}
\toprule
\multicolumn{1}{c}
{Part}           & Type \\ 
\midrule
GPU               & GTX1080TI     \\ 
CPU               & I3?      	   \\ 
RAM				  & 16 GB 			\\
\bottomrule
\end{tabular}
\end{center}
\label{tab:I}
\end{table}




\section{Results of the Inpainting}
We will first look at the inpainting data. 
Here we have 3 masks used, and 2 different networks gnererating the images. This gives us the in total six augmented datasets.



\begin{table}[h]
\centering
\caption{Details of all datasets used in the experiments.} 
\caption*{\small \textbf{BC}: Black corner. \textbf{GS}: Green square. \textbf{BC+GS}: Black corner and Green square}
\begin{center}
\begin{tabular}{lccc}
\toprule
{Dataset labels} & {Size} & {Inpainted area} & {Generator network used} \\ 
\midrule
I    - Base Case                       & 256x256 px         & -        & -                   \\
II   - Autoencoder with black corner   & 256x256 px         & BC       & Autoencoder         \\
III  - Autoencoder with green square   & 256x256 px         & GS       & Autoencoder         \\
IV   - Autoencoder with both           & 256x256 px         & BC+GS    & Autoencoder         \\
V    - GAN with black corner           & 256x256 px         & BC       & GAN                 \\
VI   - GAN with green square   		   & 256x256 px         & GS       & GAN                 \\
VII  - GAN with both                   & 256x256 px         & BC+GS    & GAN                 \\
\bottomrule
\end{tabular}%
\end{center}
\label{tab:datasets}
%\caption{\small BC: Black corner. GS: Green square. BC+GS: Black corner and Green square}
\end{table}


    \subsection{Black corners}
        \begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{experiments/figures/blackcorner/polypAE.jpg}
            \caption[Is this in use]%
            {{\small Image from the polyp class at 256x256 px, generated by an autoencoder }}    
            \label{fig:polypAEBLACK}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/blackcorner/polypGAN.jpg}
            \caption[Hate to be this guy]%
            {{\small Image from the polyp class at 256x256 px, generated by a GAN \\.}}    
            \label{fig:polypGANBLACK}
        \end{subfigure}
        \qquad\vfill%\vskip\baselineskip
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/blackcorner/zAE.jpg}
            \caption[]%
            {{\small Image from the normal-z-line class at 256x256 px, generated by an autoencoder }}    
            \label{fig:zAEBLACK}
        \end{subfigure}
        \qquad%\hfill%\quad
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/blackcorner/zGAN.jpg}
            \caption[]%
            {{\small Image from the normal-z-line class at 256x256 px, generated by a GAN }}    
            \label{fig:zGANBLACK}
        \end{subfigure}
        \caption[ ]
        {\small Two images from the polyp class and the z-line class. Images on the left were inpained with an autoencoder, and the images on the right were inpainted with a GAN} 
        \label{fig:BC1BLACK}
    \end{figure*}
    
        \begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{experiments/figures/blackcorner/ucAE.jpg}
            \caption[Is this in use]%
            {{\small Image from the ulcerative colitis class at 256x256 px, generated by an autoencoder }}    
            \label{fig:polypAEBLACK}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/blackcorner/ucGAN.jpg}
            \caption[Hate to be this guy]%
            {{\small Image from the ulcerative colitis class at 256x256 px, generated by a GAN}}    
            \label{fig:polypGANBLACK}
        \end{subfigure}
        \qquad\vfill%\vskip\baselineskip
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/blackcorner/polypwithgreenAE.jpg}
            \caption[]%
            {{\small Image from the polyp class at 256x256 px, generated by an autoencoder }}    
            \label{fig:zAEBLACK}
        \end{subfigure}
        \qquad%\hfill%\quad
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/blackcorner/polypwithgreenGAN.jpg}
            \caption[]%
            {{\small Image from the polyp class at 256x256 px, generated by a GAN\\. }}    
            \label{fig:zGANBLACK}
        \end{subfigure}
        \caption[ ]
        {\small Two images from the polyp class and the ulcerative colitis. Here we see results that are not up to a good standard with regards to to light and to green colours.} 
        \label{fig:BC2BLACK}
    \end{figure*}

\subsection{Green square}
        \begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{experiments/figures/greensquare/polypAE.png}
            \caption[Is this in use]%
            {{\small Image from the polyp class at 256x256 px, generated by an autoencoder }}    
            \label{fig:polypAEGREEN}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/greensquare/polypGAN.png}
            \caption[Hate to be this guy]%
            {{\small Image from the polyp class at 256x256 px, generated by a GAN \\.}}    
            \label{fig:polypGAN}
        \end{subfigure}
        \qquad\vfill%\vskip\baselineskip
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/greensquare/zAE.png}
            \caption[]%
            {{\small Image from the normal-z-line class at 256x256 px, generated by an autoencoder }}    
            \label{fig:zAE}
        \end{subfigure}
        \qquad%\hfill%\quad
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/greensquare/zGAN.png}
            \caption[]%
            {{\small Image from the normal-z-line class at 256x256 px, generated by a GAN }}    
            \label{fig:zGAN}
        \end{subfigure}
        \caption[ ]
        {\small New text here} 
        \label{fig:GC1GREEN}
    \end{figure*}
    
    \begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{experiments/figures/greensquare/normalmissAE.png}
            \caption[Is this in use]%
            {{\small Image from the polyp class at 256x256 px, generated by an autoencoder }}    
            \label{fig:polypAEGREEN}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/greensquare/normalmissGAN.png}
            \caption[Hate to be this guy]%
            {{\small Image from the polyp class at 256x256 px, generated by a GAN \\.}}    
            \label{fig:polypGAN}
        \end{subfigure}
        \qquad\vfill%\vskip\baselineskip
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/greensquare/polypAEN.jpg}
            \caption[]%
            {{\small Image from the normal-z-line class at 256x256 px, generated by an autoencoder }}    
            \label{fig:zAE}
        \end{subfigure}
        \qquad%\hfill%\quad
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/greensquare/polypGANN.jpg}
            \caption[]%
            {{\small Image from the normal-z-line class at 256x256 px, generated by a GAN }}    
            \label{fig:zGAN}
        \end{subfigure}
        \caption[ ]
        {\small New text here} 
        \label{fig:GC1GREEN}
    \end{figure*}
    
     

    \subsection{Combination}
        \begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{experiments/figures/both/NPAE.jpg}
            \caption[Is this in use]%
            {{\small Image from the polyp class at 256x256 px, generated by an autoencoder }}    
            \label{fig:polypAEGREEN}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/both/NPGAN.jpg}
            \caption[Hate to be this guy]%
            {{\small Image from the polyp class at 256x256 px, generated by a GAN \\.}}    
            \label{fig:polypGAN}
        \end{subfigure}
        \qquad\vfill%\vskip\baselineskip
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/both/PAE.jpg}
            \caption[]%
            {{\small Image from the polyp class at 256x256 px, generated by an autoencoder }}    
            \label{fig:zAE}
        \end{subfigure}
        \qquad%\hfill%\quad
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/both/PGAN.jpg}
            \caption[]%
            {{\small Image from the polyp class at 256x256 px, generated by a GAN }}    
            \label{fig:zGAN}
        \end{subfigure}
        \caption[ ]
        {\small New text here} 
        \label{fig:GC1GREEN}
    \end{figure*}
    
    \begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{experiments/figures/both/DLAE.jpg}
            \caption[Is this in use]%
            {{\small Image from the dye lifted polyp class at 256x256 px, generated by an autoencoder }}    
            \label{fig:polypAEGREEN}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.4\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/both/DLGAN.jpg}
            \caption[Hate to be this guy]%
            {{\small Image from the dye lifted polyp class at 256x256 px, generated by a GAN}}    
            \label{fig:polypGAN}
        \end{subfigure}
        \qquad\vfill%\vskip\baselineskip
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/both/greenAE.jpg}
            \caption[]%
            {{\small Image from the normal-z-line class at 256x256 px, generated by an autoencoder }}    
            \label{fig:zAE}
        \end{subfigure}
        \qquad%\hfill%\quad
        \begin{subfigure}[b]{0.4\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{experiments/figures/both/greenGAN.jpg}
            \caption[]%
            {{\small Image from the normal-z-line class at 256x256 px, generated by a GAN }}    
            \label{fig:zGAN}
        \end{subfigure}
        \caption[ ]
        {\small New text here} 
        \label{fig:GC1GREEN}
    \end{figure*}
    

   


\section{Results of the transfer learning experiments}



\section{summary}

















