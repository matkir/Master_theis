\section{Summary}
The task of making general models to cover a broad aspect of medical images is still a widely researched area today, and most likely it will continue to be so in the future. There are a plethora of different ways to build models for the medical domain, and in most of them, there is room for improvement. Right now, the mean age of the world population increases, and as a consequence, we perform more colonoscopies than ever before. The demand for better systems for medical diagnosis will follow this trend.

We have during this thesis presented a system that, when used correctly removes unwanted areas from the datasets. This inpainting is making the augmented datasets easier and more reliable to train, and more predisposed to be generalised.
An overlying goal was to make a system that could help with the generalisation of data, and the programs presented in the thesis we believe is a step in the right direction when it comes to the automation of medical diagnosis.

We have seen the power of inpainting when it comes to datasets previously unseen during training, both shown in our publications and the thesis.
In general, to find the right inpainting and network type is not a trivial task. Because of this, we can not recommend any method over another, as the results show that the ideal type of inpainting is closely related to the dataset used both for training and for testing.
 


\section{Contributions}
As we discussed in section \ref{cha:problemstatement}, we derived two hypothesises and a problem statement related to each of the hypothesises.
Below we restate the hypothesises and problem statements with a description of how our work answers the said statements.
We end the section by summarising our published papers, drawing connections between the thesis and the publications.\\
\clearpage
\begin{enumerate}

\item \textbf{\ref{hyp:a}:} \textit{When classifying images, we will get the best result when we have images with the least amount of sparse information. Hence, by removing areas with sparse information, we will see an increase in classification performance compared to not removing the areas.}

From our testing, we can see that inpainting sparse areas in datasets help with the classification. Throughout our testing of the datasets, when we removed sparse areas, we saw an increase in classification score. Given some exceptions, \ref{hyp:a} seems to be correct.
    


\item \textbf{\ref{que:a}:} \textit{ Can the process of inpainting of sparse areas in datasets help with training and classification performed by machine learning? If so, how detailed should the inpainting be?}\\ 
    
Inpainting areas with sparse information do help with classification. When it comes to the detail of inpainting, we do not draw any definite conclusions, but the results tend to show that a smoother form of inpainting is better.
For a dataset like the CVC 356 dataset, the dataset generated with the autoencoder gave better results than the base case when evaluated with Densenet.
Dataset like the CVC 12k dataset shows performance gain when inpainting sparse regions. This performance gain indicates that, given the notable borders in the dataset, inpainting the sparse areas work.
For datasets used for both training and testing, like Kvasir, we see too little of improvement to confirm the usage of inpainting confidently.
    


\item \textbf{\ref{hyp:b}:} \textit{When training a classifier, we will get a higher probability of generalisation of our results when removing the dataset-specific artefacts compared to not removing artefacts.}
    
Inpainting areas containing artefacts helps significantly with classification. With the right setup, we saw results both doubling and tripling the classification score. Given that our training data has artefacts that do not appear in the test set we conclude that \ref{hyp:b} is correct.


\item \textbf{\ref{que:b}:} \textit{ Can inpainting of dataset-specific artefacts help with the classification of previously unseen data done by machine learning? If so, how detailed should the inpainting be?}
    
    
Inpainting artefacts improve the classification results in most cases. When it comes to the detail of inpainting, the GAN outperformed the autoencoder in all but one of the tests. The results give us feasible suggestions that inpainting dataset-specific artefacts help with the classification of unseen data, and that the more precise the result, the better.
For the CVC 356 dataset removing artefacts within the image gave the best result for both models. When inpainting areas within the image the GAN won five out of six times, making it the best model for inpainting areas with great detail. The good results lie also in the network used. The Densenet model worked excellent for the classification of this dataset.
For the CVC 12k dataset, we saw lower scores in general, giving us indications that this dataset is notably harder to classify, and that the removal of dataset-specific artefacts does not always work flawlessly. 
As with the research question \ref{que:a} regarding sparse areas, the Kvasir dataset does not show any clear indication that the removal of dataset-specific artefacts helps.

\end{enumerate}


\noindent
To build more confidence regarding the claims stated above we also published two papers concerning the same research explored in the thesis.

\paragraph{Using preprocessing as a tool in medical image detection~\cite{Mathias2018MediaevalPaper}}
The first paper presented at the MediaEval conference in Nice, France worked exclusively on the Kvasir dataset. The result we published showed an increase in classification performance when inpainting sparse regions. 
Here we displayed that even though we tested and trained on the same dataset, we saw small performance gains. We concluded the paper with that, if the test and training set are similar to each other, we can achieve better performance gain with hyperparameter optimisation rather than preprocessing with inpainting. 


\paragraph{Unsupervised preprocessing to improve generalisation for medical image classification~\cite{Mathias2019IEEpaper}}
The second paper presented at the ISMICT conference in Oslo, Norway expanded the work presented at the MediaEval conference in 2018.
The presented result used an average of multiple runs instead of K-fold cross-reference, though we used the same datasets and transfer learning models.
Here we saw similar results as the findings presented in this thesis, only less significant. The publication presents two hypothesises that bears resemblance to \ref{hyp:a} and \ref{hyp:b}. We conclude the publication by supporting both hypothesise.






\todo{Her bør du gjenta seksjon 1.5. Få også med en mer detaljert diskusjon om HYPOTESENE holder og hva svaret er på SPØRSMÅLENE (med underspørsmål) fra seksjon 1.2. Her må du henvise tilbake til problemstillingen og vise at du har løst oppgaven du har spesifisert. Igjen si at resultater er publisert.}


\section{Future work}
The work done in this thesis shows that there might be improvements when classifying images with non-perfect information. There is still much work that can be done, both to improve inpainting and classification, and to better understand the ´´black box'' that is machine learning. 

\paragraph{Better performance GAN}
Since the start of this project, there have been published multiple new papers concerning making realistic GANs including \cite{DBLP:journals/corr/abs-1809-11096} \cite{DBLP:journals/corr/abs-1812-04948}. As this is still a relatively new research field, there are still many improvements that could be done to make the models better.
The best way to improve the GAN models is just to let them train longer. The latest model used to generate the inpainted dataset were running for approximately 40 hours, which is still away from reaching the best result. By using more time when training the models, we might achieve even better MCC when classifying the medical datasets.
Another way that might improve the GAN is better utilisation of the channel-wise fully-connected layer. In this thesis, this layer was a quintessential part of the result we got, and by tweaking the layers might give even better results.

\paragraph{Looking into using the generated images for classification}
The images generated by the GAN will most likely have features that are an essential part of the original image. If this is the same underlying features that are used in, for instance, DenseNet or Inceptionresnet we might not need to paste the inpainted area back into the original image.
Further research regarding the generator learning features from the different classes could show good results.
Another promising aspect of this is to let the discriminator guess the class in addition to real and fake images. If that is the case, the generator needs to learn features that define the different classes. We can see from images like Figure \ref{fig:p_GAN_BOTH2} that this, to a case, is already happening without making an auxiliary GAN.
In the end, the ability to compress the images with a GAN or AE might give us a new way to classify images.

\paragraph{Experiment with self attention}
We touched upon self attention in section \ref{cha:attention}. Though we did some experiments with self attention, more testing is required for a conclusion if its good or not in the context of this thesis. Future work should be to implement the attention layer into the GAN to see if the reconstructed areas can become better. 

%\paragraph{Cropping of images}
%dsgfsgsgw

\paragraph{Make a generator for new data}
We have used the GAN and AE to exclusively inpaint images, but both models can, without any extensive modifications,  generate data from the same image domain from the original dataset, just like the original DCGAN ~\cite{DBLP:journals/corr/RadfordMC15} does.  
By using the dataset to generate new previously unseen data, we might help classification not to overfit.

\paragraph{Improving the program to work cross domain}
For future work, we would like to automate the process of inpainting by making the models look better, and give the user the option of choosing their areas to inpaint. The model presented can be used at any dataset, but the user has to edit the masks manually. 

\paragraph{Using OCR to remove text}
We tested the option to remove text using an Optical Character Recognition (OCR) during this thesis, but the time used by OCR algorithms are too slow to work in real-time. Combining the system presented in this thesis with a system like Rosetta~\cite{borisyuk2018rosetta} or EAST~\cite{DBLP:journals/corr/ZhouYWWZHL17} might give a speedup, but at the conclusion of this thesis, we are not able to run OCR and classification without a multi-GPU setup.


\paragraph{NASNetLarge}
When we chose our general model for classification back in August 2018, we used InceptionResNetV2 as our model. Since then F. Chollet has added NASNetLarge~\cite{DBLP:journals/corr/ZophVSL17} to the list of transfer learning models. NASNetLarge has higher accuracy on the imagenet model and should possibly be the standard model for the classification.


